{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVID-19 Tweet Classification Challenge by #Zindi_withBERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiYrZKaHwV81",
        "colab_type": "text"
      },
      "source": [
        "This is a modification of https://github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb using the Tensorflow 2.0 Keras implementation of BERT from [kpe/bert-for-tf2](https://github.com/kpe/bert-for-tf2) with the original [google-research/bert](https://github.com/google-research/bert) weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFI2_B8ffipb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tqdm  >> /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsZvic2YxnTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Evlk1N78HIXM",
        "colab_type": "code",
        "outputId": "e8c26892-d5c8-44d8-ea31-a369e8aa18b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0-rc4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAdrQqEccIva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if tf.__version__.startswith(\"1.\"):\n",
        "  tf.enable_eager_execution()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cp5wfXDx5SPH",
        "colab_type": "text"
      },
      "source": [
        "In addition to the standard libraries we imported above, we'll need to install the [bert-for-tf2](https://github.com/kpe/bert-for-tf2) python package, and do the imports required for loading the pre-trained weights and tokenizing the input text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jviywGyWyKsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install bert-for-tf2 >> /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtI7cKWDbUVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert\n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
        "from bert.tokenization.bert_tokenization import FullTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmFYvkylMwXn",
        "colab_type": "text"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC_w8SRqN0fr",
        "colab_type": "text"
      },
      "source": [
        "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGt2yQne2tkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/content/updated_train.csv')\n",
        "test=pd.read_csv('/content/updated_test.csv')\n",
        "ss=pd.read_csv('/content/updated_ss.csv')\n",
        "\n",
        "sentences = train.text.values\n",
        "labels = train.target.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsJyoY6I-Cf8",
        "colab_type": "code",
        "outputId": "d11648b3-6624-4269-b647-60f7f94288aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1962, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wJn2W5L-Z-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z=[]\n",
        "for i in range(1962):\n",
        "  z.append(0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkY-p5z5-plC",
        "colab_type": "code",
        "outputId": "cc8b1cc2-0b84-41ec-caa7-cb65bc0d7960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df = pd.DataFrame({'target':z})\n",
        "test['target']=df \n",
        "test.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test_2</td>\n",
              "      <td>Why is  explained in the video take a look</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test_3</td>\n",
              "      <td>Ed Davey fasting for Ramadan No contest</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test_4</td>\n",
              "      <td>Is Doja Cat good or do you just miss Nicki Minaj</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test_8</td>\n",
              "      <td>How Boris Johnson s cheery wounded in action p...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test_9</td>\n",
              "      <td>Man it s terrible Not even a reason to get on ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       ID                                               text  target\n",
              "0  test_2         Why is  explained in the video take a look       0\n",
              "1  test_3            Ed Davey fasting for Ramadan No contest       0\n",
              "2  test_4   Is Doja Cat good or do you just miss Nicki Minaj       0\n",
              "3  test_8  How Boris Johnson s cheery wounded in action p...       0\n",
              "4  test_9  Man it s terrible Not even a reason to get on ...       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaE2G_2DdzVg",
        "colab_type": "text"
      },
      "source": [
        "Let's use the `MovieReviewData` class below, to prepare/encode \n",
        "the data for feeding into our BERT model, by:\n",
        "  - tokenizing the text\n",
        "  - trim or pad it to a `max_seq_len` length\n",
        "  - append the special tokens `[CLS]` and `[SEP]`\n",
        "  - convert the string tokens to numerical `ID`s using the original model's token encoding from `vocab.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8zQcXoZyEdB",
        "colab_type": "code",
        "outputId": "7ea2e423-15f7-4a1f-e030-790a7b007136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!pip install tensorflow_hub\n",
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.6/dist-packages (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (1.18.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (3.10.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (46.1.3)\n",
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.4)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqaEwdS1yV34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bert import tokenization\n",
        "import bert\n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_s_vmY33jYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetClassification:\n",
        "  DATA_COLUMN = \"text\"\n",
        "  LABEL_COLUMN = \"target\"\n",
        "\n",
        "  def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=128):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_seq_len = 0\n",
        "    self.classes = classes\n",
        "    \n",
        "    train, test = map(lambda df: df.reindex(df[TweetClassification.DATA_COLUMN].str.len().sort_values().index), [train, test])\n",
        "    \n",
        "    ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
        "\n",
        "    print(\"max seq_len\", self.max_seq_len)\n",
        "    self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
        "    self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])\n",
        "\n",
        "  def _prepare(self, df):\n",
        "    x, y = [], []\n",
        "    \n",
        "    for _, row in tqdm(df.iterrows()):\n",
        "      text, label = row[TweetClassification.DATA_COLUMN], row[TweetClassification.LABEL_COLUMN]\n",
        "      tokens = self.tokenizer.tokenize(text)\n",
        "      tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "      token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "      self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
        "      x.append(token_ids)\n",
        "      y.append(self.classes.index(label))\n",
        "\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "  def _pad(self, ids):\n",
        "    x = []\n",
        "    for input_ids in ids:\n",
        "      input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
        "      input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
        "      x.append(np.array(input_ids))\n",
        "    return np.array(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGL0mEoNFGlP",
        "colab_type": "text"
      },
      "source": [
        "## A tweak\n",
        "\n",
        "Because of a `tf.train.load_checkpoint` limitation requiring list permissions on the google storage bucket, we need to copy the pre-trained BERT weights locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_F488eixTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_ckpt_dir=\"gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/\"\n",
        "bert_ckpt_file = bert_ckpt_dir + \"bert_model.ckpt\"\n",
        "bert_config_file = bert_ckpt_dir + \"bert_config.json\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGFfkWO07cWG",
        "colab_type": "code",
        "outputId": "f425bd95-6182-4d32-a251-ce81fa9e1e2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "bert_model_dir=\"2018_10_18\"\n",
        "bert_model_name=\"uncased_L-12_H-768_A-12\"\n",
        "\n",
        "!mkdir -p .model .model/$bert_model_name\n",
        "\n",
        "for fname in [\"bert_config.json\", \"vocab.txt\", \"bert_model.ckpt.meta\", \"bert_model.ckpt.index\", \"bert_model.ckpt.data-00000-of-00001\"]:\n",
        "  cmd = f\"gsutil cp gs://bert_models/{bert_model_dir}/{bert_model_name}/{fname} .model/{bert_model_name}\"\n",
        "  !$cmd\n",
        "\n",
        "!ls -la .model .model/$bert_model_name"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_config.json...\n",
            "/ [1 files][  313.0 B/  313.0 B]                                                \n",
            "Operation completed over 1 objects/313.0 B.                                      \n",
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/vocab.txt...\n",
            "/ [1 files][226.1 KiB/226.1 KiB]                                                \n",
            "Operation completed over 1 objects/226.1 KiB.                                    \n",
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_model.ckpt.meta...\n",
            "/ [1 files][883.1 KiB/883.1 KiB]                                                \n",
            "Operation completed over 1 objects/883.1 KiB.                                    \n",
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_model.ckpt.index...\n",
            "/ [1 files][  8.3 KiB/  8.3 KiB]                                                \n",
            "Operation completed over 1 objects/8.3 KiB.                                      \n",
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001...\n",
            "/ [1 files][420.0 MiB/420.0 MiB]                                                \n",
            "Operation completed over 1 objects/420.0 MiB.                                    \n",
            ".model:\n",
            "total 12\n",
            "drwxr-xr-x 3 root root 4096 May 10 22:30 .\n",
            "drwxr-xr-x 1 root root 4096 May 10 22:30 ..\n",
            "drwxr-xr-x 2 root root 4096 May 10 22:31 uncased_L-12_H-768_A-12\n",
            "\n",
            ".model/uncased_L-12_H-768_A-12:\n",
            "total 431244\n",
            "drwxr-xr-x 2 root root      4096 May 10 22:31 .\n",
            "drwxr-xr-x 3 root root      4096 May 10 22:30 ..\n",
            "-rw-r--r-- 1 root root       313 May 10 22:30 bert_config.json\n",
            "-rw-r--r-- 1 root root 440425712 May 10 22:31 bert_model.ckpt.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root      8528 May 10 22:31 bert_model.ckpt.index\n",
            "-rw-r--r-- 1 root root    904243 May 10 22:31 bert_model.ckpt.meta\n",
            "-rw-r--r-- 1 root root    231508 May 10 22:30 vocab.txt\n",
            "CPU times: user 100 ms, sys: 52.8 ms, total: 153 ms\n",
            "Wall time: 22.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "049feT8dFprc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_ckpt_dir    = os.path.join(\".model/\",bert_model_name)\n",
        "bert_ckpt_file   = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n",
        "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4xPTleh2X2b",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the Data\n",
        "\n",
        "Now let's fetch and prepare the data by taking the first `max_seq_len` tokenens after tokenizing with the BERT tokenizer, und use `sample_size` examples for both training and testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA8WHJgzhIZf",
        "colab_type": "text"
      },
      "source": [
        "To keep training fast, we'll take a sample of about 2500 train and test examples, respectively, and use the first 128 tokens only (transformers memory and computation requirements scale quadraticly with the sequence length - so with a TPU you might use `max_seq_len=512`, but on a GPU this would be too slow, and you will have to use a very small `batch_size`s to fit the model into the GPU memory)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wo1tFQaPFds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF_3KhGQ0GTc",
        "colab_type": "code",
        "outputId": "d36a6cd1-a0d8-4451-f8db-052a3f5ef769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "classes = train.target.unique().tolist()\n",
        "\n",
        "data = TweetClassification(train,test, tokenizer, classes, max_seq_len=128)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5287it [00:02, 2231.37it/s]\n",
            "1962it [00:00, 2275.84it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max seq_len 84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfRnHSz3iSXz",
        "colab_type": "text"
      },
      "source": [
        "## Adapter BERT\n",
        "\n",
        "If we decide to use [adapter-BERT](https://arxiv.org/abs/1902.00751) we need some helpers for freezing the original BERT layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuMOGwFui4it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def flatten_layers(root_layer):\n",
        "    if isinstance(root_layer, keras.layers.Layer):\n",
        "        yield root_layer\n",
        "    for layer in root_layer._layers:\n",
        "        for sub_layer in flatten_layers(layer):\n",
        "            yield sub_layer\n",
        "\n",
        "\n",
        "def freeze_bert_layers(l_bert):\n",
        "    \"\"\"\n",
        "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n",
        "    \"\"\"\n",
        "    for layer in flatten_layers(l_bert):\n",
        "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
        "            layer.trainable = False\n",
        "        elif len(layer._layers) == 0:\n",
        "            layer.trainable = False\n",
        "        l_bert.embeddings_layer.trainable = False\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMgiYzNNPV6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
        "                                   end_learn_rate=1e-7,\n",
        "                                   warmup_epoch_count=10,\n",
        "                                   total_epoch_count=90):\n",
        "\n",
        "    def lr_scheduler(epoch):\n",
        "        if epoch < warmup_epoch_count:\n",
        "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
        "        else:\n",
        "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
        "        return float(res)\n",
        "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
        "\n",
        "    return learning_rate_scheduler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccp5trMwRtmr",
        "colab_type": "text"
      },
      "source": [
        "#Creating a model\n",
        "\n",
        "Now let's create a classification model using [adapter-BERT](https//arxiv.org/abs/1902.00751), which is clever way of reducing the trainable parameter count, by freezing the original BERT weights, and adapting them with two FFN bottlenecks (i.e. `adapter_size` bellow) in every BERT layer.\n",
        "\n",
        "**N.B.** The commented out code below show how to feed a `token_type_ids`/`segment_ids` sequence (which is not needed in our case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o2a5ZIvRcJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(max_seq_len, adapter_size=64):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  #adapter_size = 64  # see - arXiv:1902.00751\n",
        "\n",
        "  # create the bert layer\n",
        "  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
        "      bc = StockBertConfig.from_json_string(reader.read())\n",
        "      bert_params = map_stock_config_to_params(bc)\n",
        "      bert_params.adapter_size = None\n",
        "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
        "        \n",
        "  input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n",
        "  # token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"token_type_ids\")\n",
        "  # output         = bert([input_ids, token_type_ids])\n",
        "  output         = bert(input_ids)\n",
        "\n",
        "  print(\"bert shape\", output.shape)\n",
        "  cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
        "  cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
        "  #cls_out = keras.layers.Conv1D(64, 5, activation='relu')(cls_out)\n",
        "  logits = keras.layers.Dense(units=768, activation=\"relu\")(cls_out)\n",
        "  \n",
        "  logits = keras.layers.Dense(units=1, activation=\"sigmoid\")(logits)\n",
        "\n",
        "  # model = keras.Model(inputs=[input_ids, token_type_ids], outputs=logits)\n",
        "  # model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n",
        "  model = keras.Model(inputs=input_ids, outputs=logits)\n",
        "  model.build(input_shape=(None, max_seq_len))\n",
        "\n",
        "  # load the pre-trained model weights\n",
        "  load_stock_weights(bert, bert_ckpt_file)\n",
        "\n",
        "  # freeze weights if adapter-BERT is used\n",
        "  \n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(1e-5),\n",
        "                loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "  model.summary()\n",
        "        \n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbL3C6ej_OOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZnmtDc7HlEm",
        "colab_type": "code",
        "outputId": "a030be70-bfac-4dbc-945d-02e5b7adc172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "adapter_size = None # use None to fine-tune all of BERT\n",
        "model = create_model(data.max_seq_len, adapter_size=adapter_size)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert shape (None, 84, 768)\n",
            "Done loading 196 BERT weights from: .model/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f22ce2b3e10> (prefix:bert_6). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
            "Unused weights from checkpoint: \n",
            "\tbert/embeddings/token_type_embeddings\n",
            "\tbert/pooler/dense/bias\n",
            "\tbert/pooler/dense/kernel\n",
            "\tcls/predictions/output_bias\n",
            "\tcls/predictions/transform/LayerNorm/beta\n",
            "\tcls/predictions/transform/LayerNorm/gamma\n",
            "\tcls/predictions/transform/dense/bias\n",
            "\tcls/predictions/transform/dense/kernel\n",
            "\tcls/seq_relationship/output_bias\n",
            "\tcls/seq_relationship/output_weights\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_ids (InputLayer)       [(None, 84)]              0         \n",
            "_________________________________________________________________\n",
            "bert (BertModelLayer)        (None, 84, 768)           108890112 \n",
            "_________________________________________________________________\n",
            "lambda_6 (Lambda)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 768)               590592    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 769       \n",
            "=================================================================\n",
            "Total params: 109,481,473\n",
            "Trainable params: 109,481,473\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsOUnnZ0R1Jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuLOkwonF-9S",
        "colab_type": "code",
        "outputId": "c97038e4-e3f3-4f12-d5ea-3fd7f2627bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "log_dir = \".log/movie_reviews/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "\n",
        "total_epoch_count = 10\n",
        "# model.fit(x=(data.train_x, data.train_x_token_types), y=data.train_y,\n",
        "model.fit(x=data.train_x, y=data.train_y,\n",
        "          validation_split=0.2,\n",
        "          batch_size=16,\n",
        "          shuffle=True,\n",
        "          epochs=total_epoch_count,\n",
        "          callbacks=[\n",
        "                     keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
        "                     tensorboard_callback])\n",
        "\n",
        "#model.save_weights('./movie_reviews.h5', overwrite=True)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "  2/265 [..............................] - ETA: 3:37 - loss: 0.7230 - accuracy: 0.5625WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.636588). Check your callbacks.\n",
            "265/265 [==============================] - 111s 419ms/step - loss: 0.6023 - accuracy: 0.7742 - val_loss: 0.4968 - val_accuracy: 0.8960\n",
            "Epoch 2/10\n",
            "265/265 [==============================] - 106s 401ms/step - loss: 0.5474 - accuracy: 0.9083 - val_loss: 0.4910 - val_accuracy: 0.8941\n",
            "Epoch 3/10\n",
            "265/265 [==============================] - 106s 401ms/step - loss: 0.5378 - accuracy: 0.9288 - val_loss: 0.4935 - val_accuracy: 0.8913\n",
            "Epoch 4/10\n",
            "265/265 [==============================] - 106s 400ms/step - loss: 0.5327 - accuracy: 0.9416 - val_loss: 0.4981 - val_accuracy: 0.8715\n",
            "Epoch 5/10\n",
            "265/265 [==============================] - 106s 401ms/step - loss: 0.5267 - accuracy: 0.9555 - val_loss: 0.4909 - val_accuracy: 0.8960\n",
            "Epoch 6/10\n",
            "265/265 [==============================] - 105s 397ms/step - loss: 0.5257 - accuracy: 0.9586 - val_loss: 0.5203 - val_accuracy: 0.8110\n",
            "Epoch 7/10\n",
            "265/265 [==============================] - 106s 399ms/step - loss: 0.5237 - accuracy: 0.9626 - val_loss: 0.4918 - val_accuracy: 0.8913\n",
            "Epoch 8/10\n",
            "265/265 [==============================] - 106s 399ms/step - loss: 0.5203 - accuracy: 0.9702 - val_loss: 0.4890 - val_accuracy: 0.8998\n",
            "Epoch 9/10\n",
            "265/265 [==============================] - 105s 398ms/step - loss: 0.5210 - accuracy: 0.9704 - val_loss: 0.4992 - val_accuracy: 0.8696\n",
            "Epoch 10/10\n",
            "265/265 [==============================] - 106s 399ms/step - loss: 0.5199 - accuracy: 0.9707 - val_loss: 0.4860 - val_accuracy: 0.9074\n",
            "CPU times: user 21min 46s, sys: 56.3 s, total: 22min 42s\n",
            "Wall time: 18min 2s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSqMu64oHzqy",
        "colab_type": "code",
        "outputId": "6f24f757-9186-42d0-a4ec-bf9d1ca23c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "_, train_acc = model.evaluate(data.train_x, data.train_y)\n",
        "\n",
        "print(\"train acc\", train_acc)\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "166/166 [==============================] - 22s 135ms/step - loss: 0.5224 - accuracy: 0.9310\n",
            "train acc 0.9309627413749695\n",
            "CPU times: user 8.94 s, sys: 1.39 s, total: 10.3 s\n",
            "Wall time: 22.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSKDZEnVabnl",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "To evaluate the trained model, let's load the saved weights in a new model instance, and evaluate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uzdOFQ5awM1",
        "colab_type": "text"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "For prediction, we need to prepare the input text the same way as we did for training - tokenize, adding the special `[CLS]` and `[SEP]` token at begin and end of the token sequence, and pad to match the model input shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WIWHQgwYzGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "res = model.predict(data.test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAKiwfx3xKnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.DataFrame({\"ID\": ss[\"ID\"],\n",
        "\n",
        "\"target\": res[:,0]\n",
        "\n",
        "})\n",
        "\n",
        "submission.to_csv(\"submission.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoIWDBrLxSmG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "e53b8238-2e5c-4272-cd68-ec378cfab812"
      },
      "source": [
        "submission.head()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test_2</td>\n",
              "      <td>0.998830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test_3</td>\n",
              "      <td>0.999988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test_4</td>\n",
              "      <td>0.000016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test_8</td>\n",
              "      <td>0.000116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test_9</td>\n",
              "      <td>0.000229</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       ID    target\n",
              "0  test_2  0.998830\n",
              "1  test_3  0.999988\n",
              "2  test_4  0.000016\n",
              "3  test_8  0.000116\n",
              "4  test_9  0.000229"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    }
  ]
}